{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlpXs8CPzDESHaG8WsOMXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nargesmalekjani/Evaporative_Droplet/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrZpbGxHH7pw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import erfc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "D_original = 1e-9  # Original diffusion coefficient\n",
        "x_max_original = 3e-4  # Original length of the domain\n",
        "t_max_original = 4.6\n",
        "x_min_original = 1e-10\n",
        "t_min_original = 0\n",
        "\n",
        "# Normalization factors\n",
        "x_norm_factor = x_max_original - x_min_original\n",
        "t_norm_factor = t_max_original - t_min_original\n",
        "\n",
        "# Normalized parameters\n",
        "D = D_original * (t_norm_factor / x_norm_factor**2) * (x_max_original**2 / t_max_original)\n",
        "x_max = 1.0\n",
        "t_max = 1.0\n",
        "x_min = 0.0\n",
        "t_min = 0.0\n",
        "\n",
        "N_train = 500  # Number of training points\n",
        "N_bc = 100  # Number of boundary points\n",
        "\n",
        "# Define the neural network\n",
        "class PINN(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(PINN, self).__init__()\n",
        "        self.dense_layers = [tf.keras.layers.Dense(10, activation='tanh') for _ in range(3)]\n",
        "        self.output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x, t = inputs\n",
        "        x = tf.expand_dims(x, axis=-1)\n",
        "        t = tf.expand_dims(t, axis=-1)\n",
        "        c = tf.concat([x, t], axis=-1)\n",
        "        for layer in self.dense_layers:\n",
        "            c = layer(c)\n",
        "        c = self.output_layer(c)\n",
        "        return c\n",
        "\n",
        "# Initialize the PINN model\n",
        "model = PINN()\n",
        "\n",
        "# Define the loss function\n",
        "# Define the loss function\n",
        "def loss_fn(x, t):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        tape.watch(x)\n",
        "        tape.watch(t)\n",
        "        c_pred = model((x, t))\n",
        "        c_x = tape.gradient(c_pred, x)\n",
        "        c_xx = tape.gradient(c_x, x)\n",
        "    c_t = tape.gradient(c_pred, t)\n",
        "    pde_residual = c_t - D * c_xx\n",
        "\n",
        "    # Initial condition: t = 0, c = c0 = 215\n",
        "    initial_cond = model((x, tf.zeros_like(x))) - 215\n",
        "\n",
        "    # Boundary condition: x = x_max_original, c = cs = 837\n",
        "    x_bc_norm = (x_max_original - x_min_original) / x_norm_factor  # Normalize x_max_original\n",
        "    x_bc = tf.ones_like(t) * x_bc_norm\n",
        "    boundary_cond = model((x_bc, t)) - 837\n",
        "\n",
        "    loss = tf.reduce_mean(tf.square(pde_residual)) + tf.reduce_mean(tf.square(initial_cond)) + tf.reduce_mean(tf.square(boundary_cond))\n",
        "    return loss\n",
        "\n",
        "# Generate training data\n",
        "x_train = tf.random.uniform((N_train, 1), minval=x_min, maxval=x_max)\n",
        "t_train = tf.random.uniform((N_train, 1), minval=t_min, maxval=t_max)\n",
        "x_bc = tf.ones((N_bc, 1)) * x_max\n",
        "t_bc = tf.random.uniform((N_bc, 1), minval=t_min, maxval=t_max)\n",
        "\n",
        "# Compile and train the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Custom training loop\n",
        "num_epochs = 800000\n",
        "batch_size = 200\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Generate random batch of training data\n",
        "    x_batch = tf.random.uniform((batch_size, 1), minval=x_min, maxval=x_max)\n",
        "    t_batch = tf.random.uniform((batch_size, 1), minval=t_min, maxval=t_max)\n",
        "\n",
        "    # Train the model on the batch\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_fn(x_batch, t_batch)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    # Print the loss every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.numpy():.6f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "x_eval = np.linspace(x_min, x_max, 100)\n",
        "t_eval = np.linspace(t_min, t_max, 100)\n",
        "X_eval, T_eval = np.meshgrid(x_eval, t_eval)\n",
        "X_eval_flat = X_eval.flatten()\n",
        "T_eval_flat = T_eval.flatten()\n",
        "c_eval = model((X_eval_flat, T_eval_flat))\n",
        "C_eval = c_eval.numpy().reshape(X_eval.shape)\n",
        "\n",
        "# Unnormalize the solution\n",
        "X_eval_unnorm = X_eval * x_norm_factor + x_min_original\n",
        "T_eval_unnorm = T_eval * t_norm_factor + t_min_original\n",
        "\n",
        "# Plot the solution\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X_eval_unnorm, T_eval_unnorm, C_eval)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('t')\n",
        "ax.set_zlabel('C')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI_TjVecJCGC",
        "outputId": "6247b92c-19ab-4861-bee8-e14959851488"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/800000], Loss: 740021.437500\n",
            "Epoch [200/800000], Loss: 732597.125000\n",
            "Epoch [300/800000], Loss: 728974.687500\n",
            "Epoch [400/800000], Loss: 726079.750000\n",
            "Epoch [500/800000], Loss: 723424.187500\n",
            "Epoch [600/800000], Loss: 720892.750000\n",
            "Epoch [700/800000], Loss: 718433.500000\n",
            "Epoch [800/800000], Loss: 716028.687500\n",
            "Epoch [900/800000], Loss: 713659.875000\n",
            "Epoch [1000/800000], Loss: 711322.875000\n",
            "Epoch [1100/800000], Loss: 709010.812500\n",
            "Epoch [1200/800000], Loss: 706718.500000\n",
            "Epoch [1300/800000], Loss: 704445.000000\n",
            "Epoch [1400/800000], Loss: 702187.375000\n",
            "Epoch [1500/800000], Loss: 699943.500000\n",
            "Epoch [1600/800000], Loss: 697712.437500\n",
            "Epoch [1700/800000], Loss: 695493.187500\n",
            "Epoch [1800/800000], Loss: 693284.562500\n",
            "Epoch [1900/800000], Loss: 691086.312500\n",
            "Epoch [2000/800000], Loss: 688897.312500\n",
            "Epoch [2100/800000], Loss: 686717.125000\n",
            "Epoch [2200/800000], Loss: 684545.562500\n",
            "Epoch [2300/800000], Loss: 682382.000000\n",
            "Epoch [2400/800000], Loss: 680226.187500\n",
            "Epoch [2500/800000], Loss: 678077.625000\n",
            "Epoch [2600/800000], Loss: 675936.312500\n",
            "Epoch [2700/800000], Loss: 673801.937500\n",
            "Epoch [2800/800000], Loss: 671674.250000\n",
            "Epoch [2900/800000], Loss: 669553.187500\n",
            "Epoch [3000/800000], Loss: 667438.375000\n",
            "Epoch [3100/800000], Loss: 665329.750000\n",
            "Epoch [3200/800000], Loss: 663227.187500\n",
            "Epoch [3300/800000], Loss: 661130.625000\n",
            "Epoch [3400/800000], Loss: 659039.937500\n",
            "Epoch [3500/800000], Loss: 656955.062500\n",
            "Epoch [3600/800000], Loss: 654875.750000\n",
            "Epoch [3700/800000], Loss: 652802.062500\n",
            "Epoch [3800/800000], Loss: 650733.812500\n",
            "Epoch [3900/800000], Loss: 648671.062500\n",
            "Epoch [4000/800000], Loss: 646613.687500\n",
            "Epoch [4100/800000], Loss: 644561.625000\n",
            "Epoch [4200/800000], Loss: 642514.937500\n",
            "Epoch [4300/800000], Loss: 640473.312500\n",
            "Epoch [4400/800000], Loss: 638436.937500\n",
            "Epoch [4500/800000], Loss: 636405.687500\n",
            "Epoch [4600/800000], Loss: 634379.625000\n",
            "Epoch [4700/800000], Loss: 632358.625000\n",
            "Epoch [4800/800000], Loss: 630342.625000\n",
            "Epoch [4900/800000], Loss: 628331.625000\n",
            "Epoch [5000/800000], Loss: 626325.750000\n",
            "Epoch [5100/800000], Loss: 624324.750000\n",
            "Epoch [5200/800000], Loss: 622328.750000\n",
            "Epoch [5300/800000], Loss: 620337.562500\n",
            "Epoch [5400/800000], Loss: 618351.500000\n",
            "Epoch [5500/800000], Loss: 616370.250000\n",
            "Epoch [5600/800000], Loss: 614393.687500\n",
            "Epoch [5700/800000], Loss: 612422.125000\n",
            "Epoch [5800/800000], Loss: 610455.562500\n",
            "Epoch [5900/800000], Loss: 608493.812500\n",
            "Epoch [6000/800000], Loss: 606536.812500\n",
            "Epoch [6100/800000], Loss: 604584.500000\n",
            "Epoch [6200/800000], Loss: 602637.187500\n",
            "Epoch [6300/800000], Loss: 600694.875000\n",
            "Epoch [6400/800000], Loss: 598757.187500\n",
            "Epoch [6500/800000], Loss: 596824.187500\n",
            "Epoch [6600/800000], Loss: 594895.875000\n",
            "Epoch [6700/800000], Loss: 592972.437500\n",
            "Epoch [6800/800000], Loss: 591054.125000\n",
            "Epoch [6900/800000], Loss: 589140.375000\n",
            "Epoch [7000/800000], Loss: 587231.562500\n",
            "Epoch [7100/800000], Loss: 585327.062500\n",
            "Epoch [7200/800000], Loss: 583427.312500\n",
            "Epoch [7300/800000], Loss: 581532.312500\n",
            "Epoch [7400/800000], Loss: 579642.312500\n",
            "Epoch [7500/800000], Loss: 577756.812500\n",
            "Epoch [7600/800000], Loss: 575875.812500\n",
            "Epoch [7700/800000], Loss: 574000.000000\n",
            "Epoch [7800/800000], Loss: 572129.437500\n",
            "Epoch [7900/800000], Loss: 570263.625000\n",
            "Epoch [8000/800000], Loss: 568402.562500\n",
            "Epoch [8100/800000], Loss: 566546.125000\n",
            "Epoch [8200/800000], Loss: 564694.437500\n",
            "Epoch [8300/800000], Loss: 562847.187500\n",
            "Epoch [8400/800000], Loss: 561004.562500\n",
            "Epoch [8500/800000], Loss: 559166.625000\n",
            "Epoch [8600/800000], Loss: 557333.250000\n",
            "Epoch [8700/800000], Loss: 555504.500000\n",
            "Epoch [8800/800000], Loss: 553680.312500\n",
            "Epoch [8900/800000], Loss: 551860.750000\n",
            "Epoch [9000/800000], Loss: 550045.812500\n",
            "Epoch [9100/800000], Loss: 548235.687500\n",
            "Epoch [9200/800000], Loss: 546431.625000\n",
            "Epoch [9300/800000], Loss: 544632.250000\n",
            "Epoch [9400/800000], Loss: 542837.500000\n",
            "Epoch [9500/800000], Loss: 541047.312500\n",
            "Epoch [9600/800000], Loss: 539261.750000\n",
            "Epoch [9700/800000], Loss: 537480.750000\n",
            "Epoch [9800/800000], Loss: 535704.375000\n",
            "Epoch [9900/800000], Loss: 533932.625000\n",
            "Epoch [10000/800000], Loss: 532165.500000\n",
            "Epoch [10100/800000], Loss: 530402.875000\n",
            "Epoch [10200/800000], Loss: 528644.937500\n",
            "Epoch [10300/800000], Loss: 526891.500000\n",
            "Epoch [10400/800000], Loss: 525142.750000\n",
            "Epoch [10500/800000], Loss: 523398.562500\n",
            "Epoch [10600/800000], Loss: 521659.625000\n",
            "Epoch [10700/800000], Loss: 519926.343750\n",
            "Epoch [10800/800000], Loss: 518197.625000\n",
            "Epoch [10900/800000], Loss: 516473.593750\n",
            "Epoch [11000/800000], Loss: 514753.968750\n",
            "Epoch [11100/800000], Loss: 513039.125000\n",
            "Epoch [11200/800000], Loss: 511328.687500\n",
            "Epoch [11300/800000], Loss: 509623.000000\n",
            "Epoch [11400/800000], Loss: 507921.906250\n",
            "Epoch [11500/800000], Loss: 506225.218750\n",
            "Epoch [11600/800000], Loss: 504533.250000\n",
            "Epoch [11700/800000], Loss: 502845.843750\n",
            "Epoch [11800/800000], Loss: 501163.062500\n",
            "Epoch [11900/800000], Loss: 499484.781250\n",
            "Epoch [12000/800000], Loss: 497811.906250\n",
            "Epoch [12100/800000], Loss: 496144.500000\n",
            "Epoch [12200/800000], Loss: 494481.656250\n",
            "Epoch [12300/800000], Loss: 492823.375000\n",
            "Epoch [12400/800000], Loss: 491169.750000\n",
            "Epoch [12500/800000], Loss: 489520.656250\n",
            "Epoch [12600/800000], Loss: 487876.125000\n",
            "Epoch [12700/800000], Loss: 486236.218750\n",
            "Epoch [12800/800000], Loss: 484600.843750\n",
            "Epoch [12900/800000], Loss: 482970.093750\n",
            "Epoch [13000/800000], Loss: 481343.906250\n",
            "Epoch [13100/800000], Loss: 479722.250000\n",
            "Epoch [13200/800000], Loss: 478105.250000\n",
            "Epoch [13300/800000], Loss: 476493.375000\n",
            "Epoch [13400/800000], Loss: 474887.093750\n",
            "Epoch [13500/800000], Loss: 473285.406250\n",
            "Epoch [13600/800000], Loss: 471688.187500\n",
            "Epoch [13700/800000], Loss: 470095.656250\n",
            "Epoch [13800/800000], Loss: 468507.562500\n",
            "Epoch [13900/800000], Loss: 466924.125000\n",
            "Epoch [14000/800000], Loss: 465345.250000\n",
            "Epoch [14100/800000], Loss: 463770.937500\n",
            "Epoch [14200/800000], Loss: 462201.218750\n",
            "Epoch [14300/800000], Loss: 460636.062500\n",
            "Epoch [14400/800000], Loss: 459075.468750\n",
            "Epoch [14500/800000], Loss: 457519.812500\n",
            "Epoch [14600/800000], Loss: 455969.781250\n",
            "Epoch [14700/800000], Loss: 454424.531250\n",
            "Epoch [14800/800000], Loss: 452883.656250\n",
            "Epoch [14900/800000], Loss: 451347.406250\n",
            "Epoch [15000/800000], Loss: 449815.718750\n",
            "Epoch [15100/800000], Loss: 448288.593750\n",
            "Epoch [15200/800000], Loss: 446765.968750\n",
            "Epoch [15300/800000], Loss: 445248.093750\n",
            "Epoch [15400/800000], Loss: 443734.625000\n",
            "Epoch [15500/800000], Loss: 442225.781250\n",
            "Epoch [15600/800000], Loss: 440721.437500\n",
            "Epoch [15700/800000], Loss: 439222.062500\n",
            "Epoch [15800/800000], Loss: 437727.437500\n",
            "Epoch [15900/800000], Loss: 436237.343750\n",
            "Epoch [16000/800000], Loss: 434751.781250\n",
            "Epoch [16100/800000], Loss: 433270.750000\n",
            "Epoch [16200/800000], Loss: 431794.968750\n",
            "Epoch [16300/800000], Loss: 430325.343750\n",
            "Epoch [16400/800000], Loss: 428860.312500\n",
            "Epoch [16500/800000], Loss: 427399.906250\n",
            "Epoch [16600/800000], Loss: 425943.968750\n",
            "Epoch [16700/800000], Loss: 424492.625000\n",
            "Epoch [16800/800000], Loss: 423045.906250\n",
            "Epoch [16900/800000], Loss: 421603.718750\n",
            "Epoch [17000/800000], Loss: 420165.937500\n",
            "Epoch [17100/800000], Loss: 418732.718750\n",
            "Epoch [17200/800000], Loss: 417304.156250\n",
            "Epoch [17300/800000], Loss: 415880.156250\n",
            "Epoch [17400/800000], Loss: 414460.593750\n",
            "Epoch [17500/800000], Loss: 413045.593750\n",
            "Epoch [17600/800000], Loss: 411635.218750\n",
            "Epoch [17700/800000], Loss: 410229.375000\n",
            "Epoch [17800/800000], Loss: 408827.968750\n",
            "Epoch [17900/800000], Loss: 407431.093750\n",
            "Epoch [18000/800000], Loss: 406038.875000\n",
            "Epoch [18100/800000], Loss: 404651.218750\n",
            "Epoch [18200/800000], Loss: 403268.500000\n",
            "Epoch [18300/800000], Loss: 401892.562500\n",
            "Epoch [18400/800000], Loss: 400521.218750\n",
            "Epoch [18500/800000], Loss: 399154.343750\n",
            "Epoch [18600/800000], Loss: 397791.937500\n",
            "Epoch [18700/800000], Loss: 396434.125000\n",
            "Epoch [18800/800000], Loss: 395080.843750\n",
            "Epoch [18900/800000], Loss: 393732.062500\n",
            "Epoch [19000/800000], Loss: 392387.906250\n",
            "Epoch [19100/800000], Loss: 391048.187500\n",
            "Epoch [19200/800000], Loss: 389713.000000\n",
            "Epoch [19300/800000], Loss: 388382.343750\n",
            "Epoch [19400/800000], Loss: 387056.218750\n",
            "Epoch [19500/800000], Loss: 385734.562500\n",
            "Epoch [19600/800000], Loss: 384417.531250\n",
            "Epoch [19700/800000], Loss: 383105.000000\n",
            "Epoch [19800/800000], Loss: 381796.937500\n",
            "Epoch [19900/800000], Loss: 380493.468750\n",
            "Epoch [20000/800000], Loss: 379195.000000\n",
            "Epoch [20100/800000], Loss: 377903.093750\n",
            "Epoch [20200/800000], Loss: 376615.750000\n",
            "Epoch [20300/800000], Loss: 375332.875000\n",
            "Epoch [20400/800000], Loss: 374054.437500\n",
            "Epoch [20500/800000], Loss: 372780.625000\n",
            "Epoch [20600/800000], Loss: 371511.281250\n",
            "Epoch [20700/800000], Loss: 370246.406250\n",
            "Epoch [20800/800000], Loss: 368986.031250\n",
            "Epoch [20900/800000], Loss: 367730.187500\n",
            "Epoch [21000/800000], Loss: 366478.937500\n",
            "Epoch [21100/800000], Loss: 365232.093750\n",
            "Epoch [21200/800000], Loss: 363989.718750\n",
            "Epoch [21300/800000], Loss: 362751.968750\n",
            "Epoch [21400/800000], Loss: 361518.656250\n",
            "Epoch [21500/800000], Loss: 360289.843750\n",
            "Epoch [21600/800000], Loss: 359065.875000\n",
            "Epoch [21700/800000], Loss: 357848.531250\n",
            "Epoch [21800/800000], Loss: 356598.718750\n",
            "Epoch [21900/800000], Loss: 355264.750000\n",
            "Epoch [22000/800000], Loss: 353888.125000\n",
            "Epoch [22100/800000], Loss: 352525.406250\n",
            "Epoch [22200/800000], Loss: 351175.312500\n",
            "Epoch [22300/800000], Loss: 349861.437500\n",
            "Epoch [22400/800000], Loss: 348504.781250\n",
            "Epoch [22500/800000], Loss: 347174.812500\n",
            "Epoch [22600/800000], Loss: 345861.125000\n",
            "Epoch [22700/800000], Loss: 344547.968750\n",
            "Epoch [22800/800000], Loss: 343241.968750\n",
            "Epoch [22900/800000], Loss: 341926.312500\n",
            "Epoch [23000/800000], Loss: 340649.687500\n",
            "Epoch [23100/800000], Loss: 339320.718750\n",
            "Epoch [23200/800000], Loss: 338083.218750\n",
            "Epoch [23300/800000], Loss: 336763.937500\n",
            "Epoch [23400/800000], Loss: 335546.125000\n",
            "Epoch [23500/800000], Loss: 334266.500000\n",
            "Epoch [23600/800000], Loss: 332966.250000\n",
            "Epoch [23700/800000], Loss: 331785.531250\n",
            "Epoch [23800/800000], Loss: 330521.187500\n",
            "Epoch [23900/800000], Loss: 329241.031250\n",
            "Epoch [24000/800000], Loss: 328094.437500\n",
            "Epoch [24100/800000], Loss: 326825.468750\n",
            "Epoch [24200/800000], Loss: 325604.000000\n",
            "Epoch [24300/800000], Loss: 324326.593750\n",
            "Epoch [24400/800000], Loss: 323133.437500\n",
            "Epoch [24500/800000], Loss: 321922.062500\n",
            "Epoch [24600/800000], Loss: 320653.656250\n",
            "Epoch [24700/800000], Loss: 319482.406250\n",
            "Epoch [24800/800000], Loss: 318240.031250\n",
            "Epoch [24900/800000], Loss: 317173.187500\n",
            "Epoch [25000/800000], Loss: 315799.656250\n",
            "Epoch [25100/800000], Loss: 314613.125000\n",
            "Epoch [25200/800000], Loss: 313418.031250\n",
            "Epoch [25300/800000], Loss: 312251.718750\n",
            "Epoch [25400/800000], Loss: 310995.218750\n",
            "Epoch [25500/800000], Loss: 309842.750000\n",
            "Epoch [25600/800000], Loss: 308557.250000\n",
            "Epoch [25700/800000], Loss: 307357.093750\n",
            "Epoch [25800/800000], Loss: 306249.125000\n",
            "Epoch [25900/800000], Loss: 305034.750000\n",
            "Epoch [26000/800000], Loss: 303852.250000\n",
            "Epoch [26100/800000], Loss: 302661.937500\n",
            "Epoch [26200/800000], Loss: 301466.531250\n",
            "Epoch [26300/800000], Loss: 300234.875000\n",
            "Epoch [26400/800000], Loss: 299138.812500\n",
            "Epoch [26500/800000], Loss: 297940.718750\n",
            "Epoch [26600/800000], Loss: 296744.156250\n",
            "Epoch [26700/800000], Loss: 295567.656250\n",
            "Epoch [26800/800000], Loss: 294444.406250\n",
            "Epoch [26900/800000], Loss: 293296.656250\n",
            "Epoch [27000/800000], Loss: 292057.343750\n",
            "Epoch [27100/800000], Loss: 290849.000000\n",
            "Epoch [27200/800000], Loss: 289829.500000\n",
            "Epoch [27300/800000], Loss: 288689.000000\n",
            "Epoch [27400/800000], Loss: 287532.031250\n",
            "Epoch [27500/800000], Loss: 286261.500000\n",
            "Epoch [27600/800000], Loss: 285201.750000\n",
            "Epoch [27700/800000], Loss: 284002.531250\n",
            "Epoch [27800/800000], Loss: 283036.656250\n",
            "Epoch [27900/800000], Loss: 281742.656250\n",
            "Epoch [28000/800000], Loss: 280534.312500\n",
            "Epoch [28100/800000], Loss: 279433.500000\n",
            "Epoch [28200/800000], Loss: 278337.531250\n",
            "Epoch [28300/800000], Loss: 277159.687500\n",
            "Epoch [28400/800000], Loss: 276138.031250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model (using normalized x)\n",
        "x_eval_norm = np.linspace(0.0, 1.0, 100)  # Normalized x range\n",
        "t_eval_norm = np.linspace(0.0, 1.0, 100)  # Normalized t range\n",
        "X_eval_norm, T_eval_norm = np.meshgrid(x_eval_norm, t_eval_norm)\n",
        "\n",
        "# Flatten the normalized arrays for model input\n",
        "X_eval_flat = X_eval_norm.flatten()\n",
        "T_eval_flat = T_eval_norm.flatten()\n",
        "\n",
        "# Predict using the model\n",
        "c_eval = model((X_eval_flat, T_eval_flat))\n",
        "C_eval = c_eval.numpy().reshape(X_eval_norm.shape)  # Reshape to normalized grid\n",
        "\n",
        "# --- Unnormalize x for plotting ---\n",
        "X_eval_unnorm = X_eval_norm * x_norm_factor + x_min_original\n",
        "T_eval_unnorm = T_eval_norm * t_norm_factor + t_min_original\n",
        "\n",
        "# Plot the solution as a heatmap\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(C_eval, cmap='hot', origin='lower',\n",
        "               extent=[x_min_original, x_max_original, t_min_original, t_max_original],  # Original ranges\n",
        "               aspect='auto')\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('t')\n",
        "ax.set_title('Concentration Heatmap')\n",
        "\n",
        "# Add a color bar\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "cbar.ax.set_ylabel('c', rotation=-90, va=\"bottom\")\n",
        "\n",
        "# Display the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pm4YX475JCJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "C_initial = 215  # Initial concentration\n",
        "C_surface = 837  # Surface concentration\n",
        "x_max = 3e-4  # Length of half slab\n",
        "D = 1e-9  # Diffusion coefficient\n",
        "t = np.linspace(t_min, t_max, 10)  # Time range\n",
        "\n",
        "# Spatial grid\n",
        "x = np.linspace(x_min, x_max, 100)\n",
        "def evaluate_exact_solution(C_initial, C_surface, L, D, t, x):\n",
        "    C_profiles = []\n",
        "\n",
        "    for i in range(len(t)):\n",
        "        C = C_initial + (C_surface - C_initial) * erfc((x_max - x) / (2 * np.sqrt(D * t[i])))\n",
        "        C_profiles.append(C)\n",
        "\n",
        "    return C_profiles\n",
        "# Evaluate the PINN model at the same time steps and spatial points as the exact solution\n",
        "x_eval = np.linspace(x_min, x_max, 100)\n",
        "t_eval = np.linspace(t_min, t_max, 10)\n",
        "X_eval, T_eval = np.meshgrid(x_eval, t_eval)\n",
        "X_eval_flat = X_eval.flatten()\n",
        "T_eval_flat = T_eval.flatten()\n",
        "c_eval = model((X_eval_flat, T_eval_flat))\n",
        "C_eval = c_eval.numpy().reshape(X_eval.shape)\n",
        "def plot_concentration_profiles(x, exact_profiles, pinn_profiles, t):\n",
        "    for i in range(len(t)):\n",
        "        C_exact = exact_profiles[i]\n",
        "        C_pinn = pinn_profiles[i]\n",
        "        plt.plot(x, C_exact, label='Exact: t = {:.2f} s'.format(t[i]))\n",
        "        plt.plot(x, C_pinn, linestyle='--', label='PINN: t = {:.2f} s'.format(t[i]))\n",
        "\n",
        "    plt.xlabel('Position (m)')\n",
        "    plt.ylabel('Concentration (mol/mÂ³)')\n",
        "    plt.title('Concentration Profile in a Finite Half Slab')\n",
        "    plt.grid(True)\n",
        "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    plt.show()\n",
        "# Evaluate the exact solution\n",
        "exact_profiles = evaluate_exact_solution(C_initial, C_surface, x_max, D, t_eval, x_eval)\n",
        "\n",
        "# Plot the concentration profiles\n",
        "plot_concentration_profiles(x_eval, exact_profiles, C_eval, t_eval)"
      ],
      "metadata": {
        "id": "6AcRXTG6BPcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3g8ftsm3JCQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9qmxWs9LJCTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CQNGRPMaJCWW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}